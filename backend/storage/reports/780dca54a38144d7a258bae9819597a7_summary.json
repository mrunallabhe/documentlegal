{"case_id":"780dca54a38144d7a258bae9819597a7","generated_at":"2025-11-27T18:58:37.057431","timeline_events":1,"inconsistencies":0,"missing_evidence":["No CCTV footage or surveillance images found. Consider collecting CCTV from nearby locations.","Check for forensic reports (fingerprints, DNA analysis, blood spatter analysis).","No FIR (First Information Report) found. Ensure FIR is included in case files.","Obtain statements from neighbors and nearby residents.","Limited time coverage. Consider collecting evidence from extended time periods before and after the incident.","No medical reports found. If injuries occurred, obtain medical examination reports.","Collect phone records and call logs if applicable.","Verify alibis and cross-check with CCTV timestamps.","No witness statements found. Consider obtaining statements from witnesses and bystanders."],"report_path":"storage\\reports\\780dca54a38144d7a258bae9819597a7.pdf","preview":{"timeline":["Quest Journals\nJournal of Software Engineering and Simulation\nVolume 8 ~ Issue 7 (2022) pp: 44-53\nISSN(Online) :2321-3795 ISSN (Print):2321-3809\nwww.questjournals.org\nResearch Paper\nObject Detection for Crime Scene Evidence Analysis\nSule Sani\nABSTRACT\nComputer forensics is quickly becoming used for many different areas of criminal investigations. The accurate\nrecording of crime scene details is crucial to providing investigators with information which they may will assist\nin reconstructing the s..."],"inconsistencies":[],"extracted_content":[{"file_type":"document","classification":{"label":"police_memo","confidence":0.8147630989551544,"method":"BERT"},"extracted_text":"Quest Journals\nJournal of Software Engineering and Simulation\nVolume 8 ~ Issue 7 (2022) pp: 44-53\nISSN(Online) :2321-3795 ISSN (Print):2321-3809\nwww.questjournals.org\nResearch Paper\nObject Detection for Crime Scene Evidence Analysis\nSule Sani\nABSTRACT\nComputer forensics is quickly becoming used for many different areas of criminal investigations. The accurate\nrecording of crime scene details is crucial to providing investigators with information which they may will assist\nin reconstructing the scene. The key aspect of analyzing visual evidence from crime scenes is detecting instances\nof semantic objects of a certain class in digital images and videos using Machine Vision. Object detection is the\nkey module in most visual-based surveillance applications and security systems. However, due to the presence\nof a large volume of data, the task of detecting objects of interest is very tedious for law enforcement agencies.\nTherefore, this research work presents an object detection model based on Convolutional Neural Network to\ndetect objects in a crime scene without external control. The deep learning model in this research is trained on\nthe Microsoft Common Objects in Context Dataset of comprising of over 70 classes of objects.This system can\nachieve the test accuracy of 90 % for the datasets we have that is verymuch competitive with other systems for\nthis particular task.\nReceived 01 July, 2022; Revised 10 July, 2022; Accepted 12 July, 2022 © The author(s) 2022.\nPublished with open access at www.questjournals.org\n3.3 Object Detection Using Convolutional Neural Network\nWe retrain on a custom image dataset the state-of-the-art algorithm for real-time object detection\nwhich was presented in a 2016 research paper by(Redmon.) And his colleagues decided to name this algorithm\nYOLO (You Only Look Once) due to its real time object detection feature. The YOLO creators claim the\nconvolutional neural network processes images at real-time with about 45 frames per second (much faster than\nthe R-CNN and even the Faster R-CNN).\nThe YOLO algorithm divides an input image into an S X S matrix. Whereby each cell of the matrix is\na combination of pixels in the image which predicts an object. Combination of pixels or a grid cell in the\nmatrix predicts a fixed number of boundary boxes to specifically localize an object on the entire input image.\nFor each boundary box you output 5 elements (x, y, w and h) including C.\nWhere:\n(x, y) = coordinates for the center of the box (object localization) w = width of the object in\nthe image h = height of the object in the image\nC = class confidence score of the prediction\nThe major concept of the algorithm is to predict a (7, 7, 30) tensor with a CNN network having 24\nconvolutional layers and 2 fully connected layers. It predicts several boundary boxes and outputs the boxes\nwith a confidence score greater than 0.25 as the final prediction (class confidence score).\nClass confidence score = box confidence score x conditional class probability\nHowever, the algorithm still has losses (errors) when selecting the boundary box with a class confidence score\ngreater than 0.25 is calculated by the loss function.\nThe Yolo algorithm calculates the loss function as a sum-squared of three errors for each grid cell prediction\nwhich includes:\nClassification loss: Classification loss of an object detected in the input image is a squared sum error of the\nclass conditional probabilities for each class.\nPrediction accuracy of the algorithm is amazing as it uses one network for a prediction, however it can be\nretrained for layer to layer to improve its accuracy.\nYolo has various versions with different levels of accuracy when trained and tested on large datasets like\nCOCO, Image Net and so on. However, YOLO was deployed for this research because of its highly optimized\narchitecture that makes it compactible with systems of low computational abilities during the training and\ntesting phase of the model. The optimized architecture of YOLO also contributes to the feasibility of\nintegrating the model.\n*Corresponding Author: Sule Sani 44 | Page\nObject Detection for Crime Scene Evidence Analysis\n3.4 Network Design of YOLO Object Detection Model\nFurther research was conducted resulting in the December 2016 paper “YOLO9000: Better, Faster, Stronger,” by\nRedmond and Farhadi, both from the University of Washington, that provided a number of improvements to the\nYOLO detection method including the detection of over 9,000 object categories by jointly optimizing detection\nand classification.To understand the YOLO algorithm, first we need to understand what is actually being\npredicted. Ultimately, we aim to predict a class of an object and the bounding box specifying object location.\nEach bounding box can be described using four descriptors:\n1. Center of the box (bx, by)\n2. Width (bw)\n3. Height (bh)\n4. Value c corresponding to the class of an object\nAlong with that we predict a real number pc, which is the probability that there is a","entities":[{"entity":"Quest Journals\nJournal of Software Engineering and Simulation\nVolume","label":"ORG","start":0,"end":68},{"entity":"8","label":"CARDINAL","start":69,"end":70},{"entity":"2022","label":"DATE","start":82,"end":86},{"entity":"44","label":"CARDINAL","start":92,"end":94},{"entity":"2321-3795","label":"DATE","start":112,"end":121},{"entity":"ISSN","label":"ORG","start":122,"end":126},{"entity":"www.questjournals.org\nResearch Paper\nObject Detection for Crime Scene Evidence Analysis\nSule Sani\nABSTRACT\nComputer","label":"ORG","start":145,"end":260},{"entity":"Machine Vision","label":"ORG","start":665,"end":679},{"entity":"Convolutional Neural Network","label":"ORG","start":1003,"end":1031},{"entity":"Microsoft","label":"ORG","start":1152,"end":1161},{"entity":"Context Dataset","label":"PERSON","start":1180,"end":1195},{"entity":"over 70","label":"CARDINAL","start":1213,"end":1220},{"entity":"90 %","label":"PERCENT","start":1285,"end":1289},{"entity":"01 July","label":"DATE","start":1398,"end":1405},{"entity":"2022","label":"DATE","start":1407,"end":1411},{"entity":"10 July, 2022","label":"DATE","start":1421,"end":1434},{"entity":"12 July, 2022","label":"DATE","start":1445,"end":1458},{"entity":"2022","label":"DATE","start":1475,"end":1479},{"entity":"3.3 Object Detection Using Convolutional Neural Network","label":"ORG","start":1533,"end":1588},{"entity":"2016","label":"DATE","start":1713,"end":1717},{"entity":"YOLO","label":"ORG","start":1872,"end":1876},{"entity":"about 45","label":"CARDINAL","start":1960,"end":1968},{"entity":"CNN","label":"ORG","start":2037,"end":2040},{"entity":"YOLO","label":"ORG","start":2047,"end":2051},{"entity":"5","label":"CARDINAL","start":2393,"end":2394},{"entity":"C.","label":"NORP","start":2430,"end":2432},{"entity":"7","label":"DATE","start":2684,"end":2685},{"entity":"30","label":"DATE","start":2687,"end":2689},{"entity":"CNN","label":"ORG","start":2705,"end":2708},{"entity":"24","label":"CARDINAL","start":2724,"end":2726},{"entity":"2","label":"CARDINAL","start":2752,"end":2753},{"entity":"greater than 0.25","label":"CARDINAL","start":2859,"end":2876},{"entity":"0.25","label":"CARDINAL","start":3129,"end":3133},{"entity":"Yolo","label":"PERSON","start":3174,"end":3178},{"entity":"three","label":"CARDINAL","start":3238,"end":3243},{"entity":"Classification","label":"ORG","start":3297,"end":3311},{"entity":"Yolo","label":"PERSON","start":3618,"end":3622},{"entity":"YOLO","label":"ORG","start":3761,"end":3765},{"entity":"YOLO","label":"ORG","start":3994,"end":3998},{"entity":"44","label":"CARDINAL","start":4094,"end":4096},{"entity":"Page\nObject Detection for Crime Scene Evidence Analysis","label":"ORG","start":4099,"end":4154},{"entity":"3.4","label":"CARDINAL","start":4155,"end":4158},{"entity":"December 2016","label":"DATE","start":4253,"end":4266},{"entity":"Faster","label":"PERSON","start":4292,"end":4298},{"entity":"Redmond","label":"GPE","start":4314,"end":4321},{"entity":"Farhadi","label":"GPE","start":4326,"end":4333},{"entity":"the University of Washington","label":"ORG","start":4345,"end":4373},{"entity":"YOLO","label":"ORG","start":4421,"end":4425},{"entity":"over 9,000","label":"CARDINAL","start":4470,"end":4480},{"entity":"YOLO","label":"ORG","start":4568,"end":4572},{"entity":"first","label":"ORDINAL","start":4584,"end":4589},{"entity":"four","label":"CARDINAL","start":4787,"end":4791},{"entity":"1","label":"CARDINAL","start":4805,"end":4806},{"entity":"2","label":"CARDINAL","start":4835,"end":4836},{"entity":"Width","label":"PERSON","start":4838,"end":4843},{"entity":"3","label":"CARDINAL","start":4849,"end":4850},{"entity":"4","label":"CARDINAL","start":4864,"end":4865},{"entity":"YOLO doesn‟t","label":"PERSON","start":5029,"end":5041},{"entity":"3","label":"CARDINAL","start":5253,"end":5254},{"entity":"1","label":"CARDINAL","start":5256,"end":5257},{"entity":"YOLO Network Design","label":"ORG","start":5259,"end":5278},{"entity":"K=5","label":"ORG","start":5292,"end":5295},{"entity":"80","label":"CARDINAL","start":5324,"end":5326},{"entity":"YOLO","label":"ORG","start":5663,"end":5667},{"entity":"3.2","label":"CARDINAL","start":6067,"end":6070},{"entity":"45","label":"CARDINAL","start":6133,"end":6135},{"entity":"Page\nObject Detection for Crime Scene Evidence Analysis","label":"ORG","start":6138,"end":6193},{"entity":"Non-max","label":"ORG","start":6343,"end":6350},{"entity":"3.3","label":"CARDINAL","start":6558,"end":6561},{"entity":"Non-max","label":"ORG","start":6600,"end":6607},{"entity":"one","label":"CARDINAL","start":6726,"end":6729},{"entity":"3.4","label":"CARDINAL","start":6786,"end":6789},{"entity":"IoU","label":"ORG","start":6973,"end":6976},{"entity":"two","label":"CARDINAL","start":7030,"end":7033},{"entity":"3.5","label":"CARDINAL","start":7338,"end":7341},{"entity":"Non-max","label":"ORG","start":7363,"end":7370},{"entity":"46","label":"CARDINAL","start":7416,"end":7418},{"entity":"Page\nObject Detection for Crime Scene Evidence Analysis","label":"ORG","start":7421,"end":7476},{"entity":"3.6","label":"CARDINAL","start":7706,"end":7709},{"entity":"YOLO Network Architecture","label":"ORG","start":7711,"end":7736},{"entity":"YOLO Architecture","label":"ORG","start":7737,"end":7754},{"entity":"Algorithm","label":"GPE","start":7792,"end":7801},{"entity":"Loss","label":"PERSON","start":7807,"end":7811},{"entity":"YOLO","label":"ORG","start":7837,"end":7841},{"entity":"four","label":"CARDINAL","start":7878,"end":7882},{"entity":"one","label":"CARDINAL","start":8183,"end":8186},{"entity":"YOLO","label":"ORG","start":8243,"end":8247},{"entity":"220fps","label":"CARDINAL","start":8536,"end":8542},{"entity":"3.5","label":"CARDINAL","start":8544,"end":8547},{"entity":"andSuper","label":"ORG","start":8729,"end":8737},{"entity":"Microsoft","label":"ORG","start":8762,"end":8771},{"entity":"Context","label":"GPE","start":8790,"end":8797},{"entity":"91","label":"CARDINAL","start":8825,"end":8827},{"entity":"82","label":"CARDINAL","start":8858,"end":8860},{"entity":"5,000","label":"CARDINAL","start":8886,"end":8891},{"entity":"2,500,000","label":"CARDINAL","start":8936,"end":8945},{"entity":"328,000","label":"CARDINAL","start":8967,"end":8974},{"entity":"ImageNet","label":"ORG","start":9010,"end":9018},{"entity":"PASCAL","label":"ORG","start":9257,"end":9263},{"entity":"SUN","label":"ORG","start":9272,"end":9275},{"entity":"3.5.1","label":"CARDINAL","start":9457,"end":9462},{"entity":"Data Pre-processing","label":"PERSON","start":9463,"end":9482}],"objects_detected":[],"ocr_text":"","timestamps":[],"locations":[],"summary":"Quest Journals\nJournal of Software Engineering and Simulation\nVolume 8 ~ Issue 7 (2022) pp: 44-53\nISSN(Online) :2321-3795 ISSN (Print):2321-3809\nwww.questjournals.org\nResearch Paper\nObject Detection for Crime Scene Evidence Analysis\nSule Sani\nABSTRACT\nComputer forensics is quickly becoming used for many different areas of criminal investigations. The accurate\nrecording of crime scene details is crucial to providing investigators with information which they may will assist\nin reconstructing the s...","dates":[],"events":[{"event":"saw some of the algorithms that tried to solve some of these\nchallenges but were failing in the most crucial one","type":"observation","context":"th\nthe challenges we face in that domain. We then saw some of the algorithms that tried to solve some of these\nchallenges but were failing in the most crucial one-Real time detection (speed in fps). We then studi"},{"event":"observed that the\nsystem was able to detect objects with an average accuracy of","type":"observation","context":"t. While testing on the ImageNet-Room Objects, we observed that the\nsystem was able to detect objects with an average accuracy of 74.33%, where the highest accuracy was 96.2%\nobta"},{"event":"hit","type":"violence","context":" this research because of its highly optimized\narchitecture that makes it compactible with systems of l"},{"event":"hit","type":"violence","context":" and\ntesting phase of the model. The optimized architecture of YOLO also contributes to the feasibility"},{"event":"hit","type":"violence","context":"nding box of the respective class. The overall architecture of the algorithm can be viewed below:\nFigur"},{"event":"hit","type":"violence","context":" can be viewed below:\nFigure 3.6: YOLO Network Architecture\nYOLO Architecture, the most important param"},{"event":"hit","type":"violence","context":"ow:\nFigure 3.6: YOLO Network Architecture\nYOLO Architecture, the most important parameter of the Algori"},{"event":"hit","type":"violence","context":"et\n6. Libraries and Dependencies Pillow,gunicorn,WhiteNoise,Keras,Numpy,Matploit,\nOpenCV python, Imagei"},{"event":"hit","type":"violence","context":"tal Setup\nWe have used the pre-trained network architecture of YOLO to detect objects in our created te"},{"event":"hit","type":"violence","context":"nce can\nbe further improved by fine tuning the architecture using datasets with respect to each class c"},{"event":"hit","type":"violence","context":"om). We have used the state-of-the-art network architecture of YOLO algorithm, which can compute\nregion"},{"event":"left with all the different bounding boxes","type":"location_exit","context":"nd does the same process, it is\ndone until we are left with all the different bounding boxes.\nFigure 3.5: Before and After of Non-max suppress"}]}]}}